{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:37.879794Z",
     "start_time": "2019-01-07T10:00:36.050435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0-rc0\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:43.881726Z",
     "start_time": "2019-01-07T10:00:38.761201Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:43.889261Z",
     "start_time": "2019-01-07T10:00:43.885025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (25000,) (25000,)\n",
      "Test: (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", x_train.shape, y_train.shape)\n",
    "print(\"Test:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:46.979868Z",
     "start_time": "2019-01-07T10:00:46.918294Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:47.216898Z",
     "start_time": "2019-01-07T10:00:47.207277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:48.123913Z",
     "start_time": "2019-01-07T10:00:48.098413Z"
    }
   },
   "outputs": [],
   "source": [
    "keys = list(vocab.keys())\n",
    "values = list(vocab.values())\n",
    "# get a key-word mapping\n",
    "reverse_vocab = dict()\n",
    "for i in range(len(vocab)):\n",
    "    reverse_vocab[values[i]] = keys[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:49.516262Z",
     "start_time": "2019-01-07T10:00:49.512430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:49.788737Z",
     "start_time": "2019-01-07T10:00:49.784381Z"
    }
   },
   "outputs": [],
   "source": [
    "# implement method to get back the original text\n",
    "def get_original_text(vector):\n",
    "    text = list()\n",
    "    for v in vector:\n",
    "        if v in reverse_vocab:\n",
    "            text.append(reverse_vocab[v])\n",
    "        else:\n",
    "            continue\n",
    "    return \" \".join(text)\n",
    "# call\n",
    "# get_original_text(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:51.557482Z",
     "start_time": "2019-01-07T10:00:50.100250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get training reviews\n",
    "train_text = list()\n",
    "for x in x_train:\n",
    "    train_text.append(get_original_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:52.948837Z",
     "start_time": "2019-01-07T10:00:51.559606Z"
    }
   },
   "outputs": [],
   "source": [
    "# get test reviews\n",
    "test_text = list()\n",
    "for x in x_test:\n",
    "    test_text.append(get_original_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:54.335263Z",
     "start_time": "2019-01-07T10:00:54.330865Z"
    }
   },
   "outputs": [],
   "source": [
    "# create word frequency mapping\n",
    "word_frequency_mapping = dict()\n",
    "def get_word_frequency(text_list):\n",
    "    for text in text_list:\n",
    "        text = re.sub(\"[^a-zA-Z0-9 ]\", \"\", text)\n",
    "        tokens = text.split()\n",
    "        for tok in tokens:\n",
    "            if tok not in word_frequency_mapping:\n",
    "                # if not in vocab, add new word\n",
    "                word_frequency_mapping[tok] = 1\n",
    "            else:\n",
    "                # if word present in vocab, update frequency\n",
    "                word_frequency_mapping[tok] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:57.437883Z",
     "start_time": "2019-01-07T10:00:55.451022Z"
    }
   },
   "outputs": [],
   "source": [
    "# call method\n",
    "get_word_frequency(text_list=train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:57.444544Z",
     "start_time": "2019-01-07T10:00:57.440448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79341"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_frequency_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:57.501985Z",
     "start_time": "2019-01-07T10:00:57.446688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79341"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse the dict based on the values\n",
    "sorted_word_frequency_mapping = sorted(word_frequency_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "len(sorted_word_frequency_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:57.689566Z",
     "start_time": "2019-01-07T10:00:57.651651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79343"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vocabulary for the embedding\n",
    "# add word for unknown words and for padding\n",
    "word2idx = {\"<PAD>\":0, \"<UNK>\": 1}\n",
    "for i in range(len(sorted_word_frequency_mapping)):\n",
    "    word2idx[sorted_word_frequency_mapping[i][0]] = i+2\n",
    "# check for the lenght of the vocab\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:58.002060Z",
     "start_time": "2019-01-07T10:00:57.985015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79343"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a mapping from index to word\n",
    "idx2word = {val:key for key, val in word2idx.items()}\n",
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:00:58.294489Z",
     "start_time": "2019-01-07T10:00:58.289354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = \"\"\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-,_!.\"\"\"\n",
    "chars = [a for a in chars]\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:01:00.064516Z",
     "start_time": "2019-01-07T10:01:00.059376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {\"<UNK>\": 1, \"<PAD>\": 0}\n",
    "for i in range(len(chars)):\n",
    "    char2idx[chars[i]] = i+2\n",
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:01:00.339912Z",
     "start_time": "2019-01-07T10:01:00.335181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a mapping from index to word\n",
    "idx2char = {val:key for key, val in char2idx.items()}\n",
    "len(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:01:04.013432Z",
     "start_time": "2019-01-07T10:01:04.007553Z"
    }
   },
   "outputs": [],
   "source": [
    "# create character based codes for each title name\n",
    "def get_char_codes(text_list):\n",
    "    x_char = list()\n",
    "    max_len = 256\n",
    "    max_char_len = 8\n",
    "    for text in text_list:\n",
    "        text = re.sub(\"[^a-zA-Z0-9,-;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=' ]\", \"\", text)\n",
    "        tokens = text.split()\n",
    "        total_token = list()\n",
    "        for k in range(max_len):\n",
    "            word_seq = list()\n",
    "            for j in range(max_char_len):\n",
    "                try:\n",
    "                    word_seq.append(char2idx[tokens[k][j]])\n",
    "                except:\n",
    "                    word_seq.append(char2idx[\"<PAD>\"])\n",
    "            total_token.append(word_seq)\n",
    "        x_char.append(total_token)\n",
    "    return x_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:01:56.249392Z",
     "start_time": "2019-01-07T10:01:04.273319Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate char codes for both train and test reviews\n",
    "train_char = get_char_codes(text_list=train_text)\n",
    "test_char = get_char_codes(text_list=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:01:56.257508Z",
     "start_time": "2019-01-07T10:01:56.252027Z"
    }
   },
   "outputs": [],
   "source": [
    "# create word based codes for each title name\n",
    "def get_word_codes(text_list):\n",
    "    x_word = list()\n",
    "    for text in text_list:\n",
    "        text = re.sub(\"[^a-zA-Z0-9,-;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=' ]\", \"\", text)\n",
    "        tokens = text.split()\n",
    "        total_tokens=  list()\n",
    "        for tok in tokens:\n",
    "            if tok in word2idx.keys():\n",
    "                total_tokens.append(word2idx[tok])\n",
    "            else:\n",
    "                total_tokens.append(word2idx[\"<UNK>\"])\n",
    "        x_word.append(total_tokens)\n",
    "    return x_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:01.098793Z",
     "start_time": "2019-01-07T10:01:56.259767Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate word codes for both train and test reviews\n",
    "train_word = get_word_codes(text_list=train_text)\n",
    "test_word = get_word_codes(text_list=test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:01.885317Z",
     "start_time": "2019-01-07T10:02:01.101702Z"
    }
   },
   "outputs": [],
   "source": [
    "train_word = tf.keras.preprocessing.sequence.pad_sequences(train_word, maxlen=256, padding=\"post\", truncating=\"post\")\n",
    "test_word = tf.keras.preprocessing.sequence.pad_sequences(test_word, maxlen=256, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:10.669060Z",
     "start_time": "2019-01-07T10:02:01.887420Z"
    }
   },
   "outputs": [],
   "source": [
    "train_word = np.array(train_word).reshape((len(train_word), 256))\n",
    "test_word = np.array(test_word).reshape((len(test_word), 256))\n",
    "train_char = np.array(train_char).reshape((len(train_char), 256, 8))\n",
    "test_char = np.array(test_char).reshape((len(test_char), 256, 8))\n",
    "y_train = np.array(y_train).reshape((len(y_train), 1))\n",
    "y_test = np.array(y_test).reshape((len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:10.674600Z",
     "start_time": "2019-01-07T10:02:10.671030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 256), (25000, 256))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word.shape, test_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:10.681349Z",
     "start_time": "2019-01-07T10:02:10.676910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 256, 8), (25000, 256, 8))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char.shape,  test_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:12.457438Z",
     "start_time": "2019-01-07T10:02:12.452884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 1), (25000, 1))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape\n",
    "# train_label.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:36.910604Z",
     "start_time": "2019-01-07T10:02:35.974592Z"
    }
   },
   "outputs": [],
   "source": [
    "# input word tensor\n",
    "word_in = tf.keras.Input(shape=(256, ))\n",
    "emb_word = tf.keras.layers.Embedding(input_dim=len(word2idx)+2, output_dim=64, input_length=256)(word_in)\n",
    "\n",
    "# input char tensor\n",
    "char_in = tf.keras.Input(shape=(256, 8, ))\n",
    "emb_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(input_dim=len(char2idx)+2, output_dim=32, input_length=8))(char_in)\n",
    "\n",
    "# LSTM to get word encodings by character\n",
    "char_enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=512, recurrent_dropout=0.4)))(emb_char)\n",
    "\n",
    "# main BiLSTM block\n",
    "merged = tf.keras.layers.concatenate([emb_word, char_enc])\n",
    "\n",
    "# add another BiLSTM for ner task\n",
    "main_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128))(merged)\n",
    "\n",
    "# add a time distribute layer to work it in parallel\n",
    "out = tf.keras.layers.Dense(units=2, activation=\"softmax\")(main_lstm)\n",
    "\n",
    "# set the model together\n",
    "model = tf.keras.Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:38.808319Z",
     "start_time": "2019-01-07T10:02:38.713335Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:02:39.662954Z",
     "start_time": "2019-01-07T10:02:39.657187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 8)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 256, 8, 32)   2272        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 256, 64)      5078080     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 256, 1024)    2232320     time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256, 1088)    0           embedding[0][0]                  \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          1246208     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            514         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 8,559,394\n",
      "Trainable params: 8,559,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "cbk = [tf.keras.callbacks.ModelCheckpoint(filepath='imdb.weights.best.hdf5', verbose = 1, save_best_only=True, save_weights_only=False), tf.keras.callbacks.EarlyStopping(patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T10:05:01.443439Z",
     "start_time": "2019-01-07T10:05:01.440794Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:115: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.5911 - acc: 0.6813"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-eb2ec74b40d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 verbose=0)\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m               \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3050\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([train_word, train_char], y_train, batch_size=32, epochs=20, verbose=1, validation_data=([test_word, test_char], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
