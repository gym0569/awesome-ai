{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object for imdb dataset\n",
    "movie_review_data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset as train and test\n",
    "# consider only top 100,000 words for reviews\n",
    "(train_review, train_labels), (test_review, test_labels) = movie_review_data.load_data(num_words=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review), len(train_labels))\n",
    "print(len(test_review), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "print(train_review[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integers are encoded from top 100,000 frequent words, other are neglected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189\n"
     ]
    }
   ],
   "source": [
    "# we expect the reviews to be of different length\n",
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use post padding method to make all reviews of same length and if review is already greater we neglect words after the set length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word mapping for word to code\n",
    "word_index_map = movie_review_data.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network. Tensors need to be of same length. First thing we do is now to make the lengths of each of the reviews same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some space for our custom word\n",
    "for key in word_index_map.keys():\n",
    "    word_index_map[key] += 1\n",
    "# add padding as value 0\n",
    "word_index_map[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max review length as 256\n",
    "# set the length of all the reviews\n",
    "train_review = keras.preprocessing.sequence.pad_sequences(train_review,\n",
    "                                                          value = word_index_map[\"<PAD>\"],\n",
    "                                                          maxlen = 256,\n",
    "                                                          padding = \"post\"\n",
    "                                                         )\n",
    "test_review = keras.preprocessing.sequence.pad_sequences(test_review,\n",
    "                                                         value = word_index_map[\"<PAD>\"],\n",
    "                                                         maxlen = 256,\n",
    "                                                         padding = \"post\"\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "# now check for review length\n",
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WALLAH!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review representation as seen by our network\n",
    "train_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "# create validation set\n",
    "val_review = test_review[:10000]\n",
    "val_labels = test_labels[:10000]\n",
    "\n",
    "# update test set\n",
    "test_review = test_review[10000:]\n",
    "test_labels = test_labels[10000:]\n",
    "\n",
    "# check the size of validations set\n",
    "print(len(val_review), len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size - most frequent words\n",
    "vocab_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom callback to monitor model performance\n",
    "# stop the model if validation loss increases for 3 consecutive epochs\n",
    "cbk = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          3200000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,200,545\n",
      "Trainable params: 3,200,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.6932 - acc: 0.5205 - val_loss: 0.6903 - val_acc: 0.5415\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 1s 51us/step - loss: 0.6874 - acc: 0.6243 - val_loss: 0.6860 - val_acc: 0.5123\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.6806 - acc: 0.6356 - val_loss: 0.6769 - val_acc: 0.7328\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.6679 - acc: 0.7396 - val_loss: 0.6619 - val_acc: 0.7252\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.6448 - acc: 0.7709 - val_loss: 0.6346 - val_acc: 0.7490\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.6055 - acc: 0.7962 - val_loss: 0.5918 - val_acc: 0.7833\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.5503 - acc: 0.8154 - val_loss: 0.5392 - val_acc: 0.8015\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.4884 - acc: 0.8347 - val_loss: 0.4872 - val_acc: 0.8197\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.4298 - acc: 0.8550 - val_loss: 0.4416 - val_acc: 0.8335\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.3800 - acc: 0.8722 - val_loss: 0.4055 - val_acc: 0.8441\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.3385 - acc: 0.8874 - val_loss: 0.3773 - val_acc: 0.8542\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 1s 50us/step - loss: 0.3051 - acc: 0.8982 - val_loss: 0.3571 - val_acc: 0.8615\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.2780 - acc: 0.9068 - val_loss: 0.3408 - val_acc: 0.8649\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.2559 - acc: 0.9139 - val_loss: 0.3288 - val_acc: 0.8674\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 1s 52us/step - loss: 0.2367 - acc: 0.9214 - val_loss: 0.3194 - val_acc: 0.8722\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.2198 - acc: 0.9274 - val_loss: 0.3115 - val_acc: 0.8739\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.2056 - acc: 0.9318 - val_loss: 0.3062 - val_acc: 0.8767\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.1926 - acc: 0.9374 - val_loss: 0.3018 - val_acc: 0.8766\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.1805 - acc: 0.9423 - val_loss: 0.2973 - val_acc: 0.8792\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.1694 - acc: 0.9459 - val_loss: 0.2953 - val_acc: 0.8785\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.1596 - acc: 0.9496 - val_loss: 0.2923 - val_acc: 0.8796\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 1s 55us/step - loss: 0.1505 - acc: 0.9542 - val_loss: 0.2934 - val_acc: 0.8788\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.1421 - acc: 0.9561 - val_loss: 0.2911 - val_acc: 0.8797\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.1344 - acc: 0.9592 - val_loss: 0.2899 - val_acc: 0.8811\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.1266 - acc: 0.9623 - val_loss: 0.2892 - val_acc: 0.8807\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.1196 - acc: 0.9657 - val_loss: 0.2897 - val_acc: 0.8807\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.1132 - acc: 0.9682 - val_loss: 0.2905 - val_acc: 0.8805\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.1070 - acc: 0.9706 - val_loss: 0.2914 - val_acc: 0.8796\n",
      "15000/15000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2938540789842606, 0.8829333333333333]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 32), # get embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create 1d vector\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # 1 node with sigmoid activation\n",
    "])\n",
    "\n",
    "# get model configuration\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # using adaptive learning rate\n",
    "              loss = keras.losses.binary_crossentropy, # loss function\n",
    "              metrics = [\"accuracy\"]) # evaluation metrics\n",
    "\n",
    "# fit the compiled model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # max number of epochs\n",
    "          batch_size = 512, # number of reveiws taken at a time\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# evaluate the model on our test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is overfitted. Let us create a bigger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 32)          3200000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,202,177\n",
      "Trainable params: 3,202,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 3s 105us/step - loss: 0.6916 - acc: 0.5125 - val_loss: 0.6886 - val_acc: 0.5027\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 2s 66us/step - loss: 0.6832 - acc: 0.5987 - val_loss: 0.6784 - val_acc: 0.6689\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 2s 66us/step - loss: 0.6679 - acc: 0.7206 - val_loss: 0.6588 - val_acc: 0.7463\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.6356 - acc: 0.7674 - val_loss: 0.6190 - val_acc: 0.7425\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 2s 67us/step - loss: 0.5771 - acc: 0.7934 - val_loss: 0.5573 - val_acc: 0.7899\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.4999 - acc: 0.8277 - val_loss: 0.4889 - val_acc: 0.8105\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.4239 - acc: 0.8550 - val_loss: 0.4306 - val_acc: 0.8369\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.3617 - acc: 0.8770 - val_loss: 0.3883 - val_acc: 0.8503\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.3144 - acc: 0.8930 - val_loss: 0.3596 - val_acc: 0.8581\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.2781 - acc: 0.9070 - val_loss: 0.3387 - val_acc: 0.8652\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 2s 80us/step - loss: 0.2497 - acc: 0.9153 - val_loss: 0.3256 - val_acc: 0.8676\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.2268 - acc: 0.9250 - val_loss: 0.3137 - val_acc: 0.8719\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 2s 99us/step - loss: 0.2066 - acc: 0.9322 - val_loss: 0.3064 - val_acc: 0.8741\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.1895 - acc: 0.9380 - val_loss: 0.3016 - val_acc: 0.8742\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 2s 95us/step - loss: 0.1742 - acc: 0.9444 - val_loss: 0.2969 - val_acc: 0.8777\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 2s 88us/step - loss: 0.1611 - acc: 0.9490 - val_loss: 0.2936 - val_acc: 0.8791\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 2s 62us/step - loss: 0.1490 - acc: 0.9542 - val_loss: 0.2930 - val_acc: 0.8801\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 2s 60us/step - loss: 0.1377 - acc: 0.9579 - val_loss: 0.2908 - val_acc: 0.8825\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.1277 - acc: 0.9618 - val_loss: 0.2917 - val_acc: 0.8832\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.1182 - acc: 0.9658 - val_loss: 0.2912 - val_acc: 0.8833\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 2s 79us/step - loss: 0.1101 - acc: 0.9692 - val_loss: 0.2920 - val_acc: 0.8822\n",
      "15000/15000 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2936652056773504, 0.8821333333651225]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 32), # get embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create 1d vector\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # 1 node with sigmoid activation\n",
    "])\n",
    "\n",
    "# get model configuration\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # using adaptive learning rate\n",
    "              loss = keras.losses.binary_crossentropy, # loss function\n",
    "              metrics = [\"accuracy\"]) # evaluation metrics\n",
    "\n",
    "# fit the compiled model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # max number of epochs\n",
    "          batch_size = 512, # number of reveiws taken at a time\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# evaluate the model on our test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement. Lets add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 32)          3200000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_12  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,206,337\n",
      "Trainable params: 3,206,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 3s 109us/step - loss: 0.6922 - acc: 0.5242 - val_loss: 0.6901 - val_acc: 0.5175\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.6862 - acc: 0.5550 - val_loss: 0.6790 - val_acc: 0.6083\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.6573 - acc: 0.6874 - val_loss: 0.6273 - val_acc: 0.7401\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.5593 - acc: 0.7726 - val_loss: 0.5153 - val_acc: 0.7609\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.4223 - acc: 0.8314 - val_loss: 0.4078 - val_acc: 0.8316\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 2s 85us/step - loss: 0.3202 - acc: 0.8798 - val_loss: 0.3491 - val_acc: 0.8576\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.2606 - acc: 0.9027 - val_loss: 0.3382 - val_acc: 0.8528\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 2s 62us/step - loss: 0.2205 - acc: 0.9203 - val_loss: 0.3101 - val_acc: 0.8698\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 2s 64us/step - loss: 0.1902 - acc: 0.9324 - val_loss: 0.3033 - val_acc: 0.8756\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.1657 - acc: 0.9418 - val_loss: 0.3103 - val_acc: 0.8682\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.1448 - acc: 0.9510 - val_loss: 0.2995 - val_acc: 0.8766\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 4s 141us/step - loss: 0.1265 - acc: 0.9584 - val_loss: 0.2990 - val_acc: 0.8801\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 3s 136us/step - loss: 0.1125 - acc: 0.9640 - val_loss: 0.3019 - val_acc: 0.8797\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 4s 141us/step - loss: 0.0991 - acc: 0.9690 - val_loss: 0.3098 - val_acc: 0.8763\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 4s 146us/step - loss: 0.0863 - acc: 0.9746 - val_loss: 0.3179 - val_acc: 0.8748\n",
      "15000/15000 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3218144570748011, 0.8778666666666667]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 32), # get embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create 1d vector\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # 1 node with sigmoid activation\n",
    "])\n",
    "\n",
    "# get model configuration\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # using adaptive learning rate\n",
    "              loss = keras.losses.binary_crossentropy, # loss function\n",
    "              metrics = [\"accuracy\"]) # evaluation metrics\n",
    "\n",
    "# fit the compiled model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # max number of epochs\n",
    "          batch_size = 512, # number of reveiws taken at a time\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# evaluate the model on our test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope nothing.  We go back to the simpler model and try to overcome overfitting using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, None, 32)          3200000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_14  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,200,545\n",
      "Trainable params: 3,200,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 3s 126us/step - loss: 0.6966 - acc: 0.5042 - val_loss: 0.6925 - val_acc: 0.5087\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 2s 69us/step - loss: 0.6898 - acc: 0.5334 - val_loss: 0.6870 - val_acc: 0.6108\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.6825 - acc: 0.5900 - val_loss: 0.6788 - val_acc: 0.6624\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 2s 73us/step - loss: 0.6723 - acc: 0.6277 - val_loss: 0.6672 - val_acc: 0.7626\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 2s 76us/step - loss: 0.6589 - acc: 0.6532 - val_loss: 0.6524 - val_acc: 0.7817\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.6408 - acc: 0.6710 - val_loss: 0.6318 - val_acc: 0.7678\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 2s 80us/step - loss: 0.6180 - acc: 0.6927 - val_loss: 0.6070 - val_acc: 0.7890\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.5886 - acc: 0.7144 - val_loss: 0.5770 - val_acc: 0.8127\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.5614 - acc: 0.7343 - val_loss: 0.5461 - val_acc: 0.8257\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 2s 96us/step - loss: 0.5321 - acc: 0.7477 - val_loss: 0.5168 - val_acc: 0.8353\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 2s 79us/step - loss: 0.5039 - acc: 0.7631 - val_loss: 0.4885 - val_acc: 0.8439\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 2s 83us/step - loss: 0.4789 - acc: 0.7702 - val_loss: 0.4646 - val_acc: 0.8481\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 2s 79us/step - loss: 0.4518 - acc: 0.7856 - val_loss: 0.4376 - val_acc: 0.8542\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 2s 80us/step - loss: 0.4274 - acc: 0.7983 - val_loss: 0.4185 - val_acc: 0.8570\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 2s 82us/step - loss: 0.4086 - acc: 0.8052 - val_loss: 0.3991 - val_acc: 0.8624\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 2s 87us/step - loss: 0.3898 - acc: 0.8159 - val_loss: 0.3868 - val_acc: 0.8659\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.3791 - acc: 0.8156 - val_loss: 0.3729 - val_acc: 0.8662\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 2s 78us/step - loss: 0.3630 - acc: 0.8256 - val_loss: 0.3598 - val_acc: 0.8707\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.3493 - acc: 0.8305 - val_loss: 0.3514 - val_acc: 0.8732\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 2s 85us/step - loss: 0.3409 - acc: 0.8338 - val_loss: 0.3420 - val_acc: 0.8754\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 2s 82us/step - loss: 0.3271 - acc: 0.8444 - val_loss: 0.3327 - val_acc: 0.8756\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 2s 83us/step - loss: 0.3175 - acc: 0.8442 - val_loss: 0.3286 - val_acc: 0.8737\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 2s 82us/step - loss: 0.3084 - acc: 0.8513 - val_loss: 0.3217 - val_acc: 0.8765\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 2s 82us/step - loss: 0.3004 - acc: 0.8519 - val_loss: 0.3168 - val_acc: 0.8781\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 2s 81us/step - loss: 0.2926 - acc: 0.8550 - val_loss: 0.3123 - val_acc: 0.8788\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 2s 79us/step - loss: 0.2847 - acc: 0.8595 - val_loss: 0.3078 - val_acc: 0.8791\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.2775 - acc: 0.8620 - val_loss: 0.3040 - val_acc: 0.8785\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 2s 75us/step - loss: 0.2691 - acc: 0.8630 - val_loss: 0.3001 - val_acc: 0.8802\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 2s 77us/step - loss: 0.2645 - acc: 0.8642 - val_loss: 0.3007 - val_acc: 0.8791\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 2s 89us/step - loss: 0.2565 - acc: 0.8696 - val_loss: 0.2972 - val_acc: 0.8808\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 2s 78us/step - loss: 0.2544 - acc: 0.8670 - val_loss: 0.2946 - val_acc: 0.8813\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 2s 69us/step - loss: 0.2450 - acc: 0.8745 - val_loss: 0.2922 - val_acc: 0.8807\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.2414 - acc: 0.8752 - val_loss: 0.2908 - val_acc: 0.8814\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.2347 - acc: 0.8786 - val_loss: 0.2903 - val_acc: 0.8817\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 2s 69us/step - loss: 0.2340 - acc: 0.8784 - val_loss: 0.2878 - val_acc: 0.8812\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 2s 69us/step - loss: 0.2267 - acc: 0.8806 - val_loss: 0.2877 - val_acc: 0.8818\n",
      "Epoch 37/100\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.2234 - acc: 0.8816 - val_loss: 0.2905 - val_acc: 0.8795\n",
      "Epoch 38/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.2180 - acc: 0.8856 - val_loss: 0.2868 - val_acc: 0.8827\n",
      "Epoch 39/100\n",
      "25000/25000 [==============================] - 2s 72us/step - loss: 0.2153 - acc: 0.8861 - val_loss: 0.2867 - val_acc: 0.8826\n",
      "Epoch 40/100\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.2059 - acc: 0.8894 - val_loss: 0.2886 - val_acc: 0.8808\n",
      "Epoch 41/100\n",
      "25000/25000 [==============================] - 2s 80us/step - loss: 0.2077 - acc: 0.8874 - val_loss: 0.2872 - val_acc: 0.8814\n",
      "Epoch 42/100\n",
      "25000/25000 [==============================] - 2s 68us/step - loss: 0.2019 - acc: 0.8922 - val_loss: 0.2869 - val_acc: 0.8829\n",
      "15000/15000 [==============================] - 0s 18us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28850850370724995, 0.8842000000317891]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 32), # get embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create 1d vector\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # 1 node with sigmoid activation\n",
    "])\n",
    "\n",
    "# get model configuration\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # using adaptive learning rate\n",
    "              loss = keras.losses.binary_crossentropy, # loss function\n",
    "              metrics = [\"accuracy\"]) # evaluation metrics\n",
    "\n",
    "# fit the compiled model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # max number of epochs\n",
    "          batch_size = 512, # number of reveiws taken at a time\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# evaluate the model on our test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, None, 128)         12800000  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_29  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 12,802,081\n",
      "Trainable params: 12,802,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 19s 754us/step - loss: 0.6916 - acc: 0.5486 - val_loss: 0.6733 - val_acc: 0.7761\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 18s 728us/step - loss: 0.6538 - acc: 0.6486 - val_loss: 0.6266 - val_acc: 0.8064\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 17s 672us/step - loss: 0.5898 - acc: 0.7030 - val_loss: 0.5456 - val_acc: 0.8096\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 17s 694us/step - loss: 0.5165 - acc: 0.7552 - val_loss: 0.4625 - val_acc: 0.8576\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 14s 562us/step - loss: 0.4515 - acc: 0.7943 - val_loss: 0.3975 - val_acc: 0.8689\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 14s 562us/step - loss: 0.4120 - acc: 0.8180 - val_loss: 0.3663 - val_acc: 0.8766\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 15s 617us/step - loss: 0.3787 - acc: 0.8345 - val_loss: 0.3402 - val_acc: 0.8816\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 14s 569us/step - loss: 0.3527 - acc: 0.8475 - val_loss: 0.3184 - val_acc: 0.8807\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 15s 609us/step - loss: 0.3313 - acc: 0.8574 - val_loss: 0.3129 - val_acc: 0.8841\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 16s 658us/step - loss: 0.3158 - acc: 0.8646 - val_loss: 0.3013 - val_acc: 0.8847\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 20s 819us/step - loss: 0.2996 - acc: 0.8711 - val_loss: 0.2928 - val_acc: 0.8815\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 19s 754us/step - loss: 0.2860 - acc: 0.8784 - val_loss: 0.2883 - val_acc: 0.8850\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 19s 762us/step - loss: 0.2743 - acc: 0.8848 - val_loss: 0.2857 - val_acc: 0.8843\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 17s 677us/step - loss: 0.2644 - acc: 0.8903 - val_loss: 0.2831 - val_acc: 0.8834\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 17s 678us/step - loss: 0.2567 - acc: 0.8919 - val_loss: 0.2981 - val_acc: 0.8786\n",
      "Epoch 16/100\n",
      "20736/25000 [=======================>......] - ETA: 3s - loss: 0.2489 - acc: 0.8957"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-640d6bf9e09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# number of reveiws taken at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           callbacks = cbk)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# evaluate the model on our test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 128), # get embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create 1d vector\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu), # 16 nodes with relu\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # 1 node with sigmoid activation\n",
    "])\n",
    "\n",
    "# get model configuration\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # using adaptive learning rate\n",
    "              loss = keras.losses.binary_crossentropy, # loss function\n",
    "              metrics = [\"accuracy\"]) # evaluation metrics\n",
    "\n",
    "# fit the compiled model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # max number of epochs\n",
    "          batch_size = 128, # number of reveiws taken at a time\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# evaluate the model on our test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
