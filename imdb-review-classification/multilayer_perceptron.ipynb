{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA FROM KERAS DATASETS\n",
    "movie_review_data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE BUCKETS FOR TRAIN AND TEST\n",
    "# CONSIDER ONLY TOP 100,000 WORDS FOR REVIEWS\n",
    "(train_review, train_labels), (test_review, test_labels) = movie_review_data.load_data(num_words=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review), len(train_labels))\n",
    "print(len(test_review), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "# VIEW OUR DATA IN HAND\n",
    "print(train_review[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE INTEGERS HERE ARE ENCODED TOP 100,000 WORDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189\n"
     ]
    }
   ],
   "source": [
    "# USING DOMAIN KNOWLEDGE WE KNOW THAT REVIEW LENGTHS MAY OR MAY NOT BE SAME\n",
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS EXPECTED! WE NEED TO MAKE ALL AREVIEWS OF SAME LENGTH SO THAT OUR NETWORK CAN HANDLE THEM. WE USE A METHOD CALLED POST PADDING. WE ADD EXTRA PADDING AT THE END OF EACH REVIEW IF LESS THAN A SET LENGTH OR DISCARD EVERYTHING AFTER THAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET WORD MAP WHICH IS INBUILT IN KERAS DATASETS\n",
    "word_index_map = movie_review_data.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "# DICTIONARY OF WORD AND CODE MAPPING\n",
    "print(len(word_index_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network. Tensors need to be of same length. First thing we do is now to make the lengths of each of the reviews same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88585"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHIFT THE DICTIONARY ELEMENTS BY ONE PLACE\n",
    "for key in word_index_map.keys():\n",
    "    word_index_map[key] += 1\n",
    "# ADD NEW PADDING SYMBOL WITH VALUE 0\n",
    "word_index_map[\"<PAD>\"] = 0\n",
    "\n",
    "# CHECK FOR LENGHT OF THE MAPPING\n",
    "len(word_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US MAKE ALL REVIEWS OF SAME LENGHT USING PADDING\n",
    "# SET MAX REVIEW LENGTH AS 256 WORDS\n",
    "# IF REVIEW GREATER THAN THAT TRUNCATE IT ELSE ADD PADDING\n",
    "train_review = keras.preprocessing.sequence.pad_sequences(train_review,\n",
    "                                                          value = word_index_map[\"<PAD>\"],\n",
    "                                                          maxlen = 256,\n",
    "                                                          padding = \"post\"\n",
    "                                                         )\n",
    "# DO THE SAME FOR TEST\n",
    "test_review = keras.preprocessing.sequence.pad_sequences(test_review,\n",
    "                                                         value = word_index_map[\"<PAD>\"],\n",
    "                                                         maxlen = 256,\n",
    "                                                         padding = \"post\"\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "# CHECK THE REVIEW LENGTH AGAIN\n",
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WALLAH!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REVIEW REPRESENTATION AS SEEN BY OUR NETWORK\n",
    "train_review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA PADDING IS ADDED AT THE END OF REVIEW. NOW WE ARE READY TO TRAIN OUR MODEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# CREATE VALIDATION SET\n",
    "val_review = train_review[:5000]\n",
    "val_labels = train_labels[:5000]\n",
    "# UDPATE TRAIN DATASET\n",
    "train_review = train_review[5000:]\n",
    "train_labels = train_labels[5000:]\n",
    "\n",
    "print(len(val_review), len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER ONLY TOP 100,000 WORDS AS SPECIFIED BEFORE\n",
    "vocab_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM CALL BACK TO STOP IF VALIDATION LOSS IS INCREASING FOR 3 CONTINUOUS EPOCH\n",
    "cbk = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, None, 32)          3200000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_41  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,201,089\n",
      "Trainable params: 3,201,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 14s 680us/step - loss: 0.6734 - acc: 0.6114 - val_loss: 0.6292 - val_acc: 0.6676\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 10s 478us/step - loss: 0.4829 - acc: 0.8145 - val_loss: 0.3866 - val_acc: 0.8512\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 10s 507us/step - loss: 0.3021 - acc: 0.8879 - val_loss: 0.3129 - val_acc: 0.8760\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 10s 481us/step - loss: 0.2314 - acc: 0.9134 - val_loss: 0.2932 - val_acc: 0.8810\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 11s 567us/step - loss: 0.1868 - acc: 0.9334 - val_loss: 0.2748 - val_acc: 0.8948\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 10s 515us/step - loss: 0.1545 - acc: 0.9478 - val_loss: 0.2743 - val_acc: 0.8912\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 10s 482us/step - loss: 0.1260 - acc: 0.9600 - val_loss: 0.2757 - val_acc: 0.8966\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 10s 491us/step - loss: 0.1030 - acc: 0.9689 - val_loss: 0.2781 - val_acc: 0.8988\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 10s 484us/step - loss: 0.0843 - acc: 0.9755 - val_loss: 0.2872 - val_acc: 0.8962\n",
      "25000/25000 [==============================] - 1s 42us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3218516930294037, 0.87828]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DESIGN SEQUENTIAL MULTI-LAYER PERCEPTRON NETWORK\n",
    "# START WITH A SIMPLE NETWORK\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 32), # LOOK FOR EMBEDDINGS - 16 NODES\n",
    "    keras.layers.GlobalAvgPool1D(), # CREATE 1D VECTOR USIGN EMBEDDING\n",
    "    \n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # FIRST HIDDEN LAYER WITH 32 NODES\n",
    "    \n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # USING SIGMOID FOR ACTIVATION\n",
    "])\n",
    "\n",
    "# GET THE MODEL CONFIGURATION\n",
    "model.summary()\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # USING ADAPTIVE LEARNING RATE\n",
    "              loss = keras.losses.binary_crossentropy, # CATAGORICAL LOSS FUNCTION\n",
    "              metrics = [\"accuracy\"]) # ACCURACY AS OUR METRIC FOR EVALUATION\n",
    "\n",
    "# FIT MODEL TO TRAIN DATA\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # SET MAX EPOCH\n",
    "          batch_size = 32, # NUMBER OF REVIEWS AT ONCE TO TRAIN\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# EVALUATE MODEL ON TEST DATA\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCENT PERFORMANCE BUT MODEL IS OVERFITTED. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, None, 64)          6400000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_51  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 6,403,169\n",
      "Trainable params: 6,403,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 9s 442us/step - loss: 0.7103 - acc: 0.5055 - val_loss: 0.6930 - val_acc: 0.4978\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 6s 309us/step - loss: 0.6914 - acc: 0.5274 - val_loss: 0.6914 - val_acc: 0.5932\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 6s 309us/step - loss: 0.6885 - acc: 0.5354 - val_loss: 0.6844 - val_acc: 0.6950\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 6s 312us/step - loss: 0.6762 - acc: 0.5627 - val_loss: 0.6561 - val_acc: 0.8044\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 6s 311us/step - loss: 0.6602 - acc: 0.5849 - val_loss: 0.6447 - val_acc: 0.7976\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 7s 326us/step - loss: 0.6434 - acc: 0.6052 - val_loss: 0.6117 - val_acc: 0.8380\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 6s 314us/step - loss: 0.6319 - acc: 0.6152 - val_loss: 0.5639 - val_acc: 0.8650\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 7s 333us/step - loss: 0.6148 - acc: 0.6533 - val_loss: 0.5629 - val_acc: 0.8378\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 0.5830 - acc: 0.6941 - val_loss: 0.5040 - val_acc: 0.8672\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 0.5680 - acc: 0.7005 - val_loss: 0.4651 - val_acc: 0.8792\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 6s 318us/step - loss: 0.5530 - acc: 0.7101 - val_loss: 0.4831 - val_acc: 0.8512\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 6s 318us/step - loss: 0.5465 - acc: 0.7129 - val_loss: 0.4304 - val_acc: 0.8848\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 6s 319us/step - loss: 0.5358 - acc: 0.7212 - val_loss: 0.4138 - val_acc: 0.8762\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 6s 321us/step - loss: 0.5324 - acc: 0.7216 - val_loss: 0.4099 - val_acc: 0.8826\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 6s 321us/step - loss: 0.5279 - acc: 0.7222 - val_loss: 0.4089 - val_acc: 0.8826\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 7s 326us/step - loss: 0.5175 - acc: 0.7328 - val_loss: 0.4006 - val_acc: 0.8770\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 6s 324us/step - loss: 0.5199 - acc: 0.7274 - val_loss: 0.4003 - val_acc: 0.8850\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 7s 329us/step - loss: 0.5160 - acc: 0.7303 - val_loss: 0.3982 - val_acc: 0.8844\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 7s 328us/step - loss: 0.5099 - acc: 0.7339 - val_loss: 0.3961 - val_acc: 0.8774\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 7s 332us/step - loss: 0.5084 - acc: 0.7341 - val_loss: 0.3951 - val_acc: 0.8842\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 7s 328us/step - loss: 0.5112 - acc: 0.7312 - val_loss: 0.3936 - val_acc: 0.8876\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 7s 333us/step - loss: 0.5055 - acc: 0.7347 - val_loss: 0.3888 - val_acc: 0.8862\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 7s 331us/step - loss: 0.4976 - acc: 0.7409 - val_loss: 0.3969 - val_acc: 0.8712\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 7s 336us/step - loss: 0.5028 - acc: 0.7359 - val_loss: 0.3904 - val_acc: 0.8826\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 7s 339us/step - loss: 0.5025 - acc: 0.7372 - val_loss: 0.4114 - val_acc: 0.8818\n",
      "25000/25000 [==============================] - 2s 62us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42884352968215944, 0.8726]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DESIGN SEQUENTIAL MULTI-LAYER PERCEPTRON NETWORK\n",
    "# START WITH A SIMPLE NETWORK\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 64), # LOOK FOR EMBEDDINGS - 16 NODES\n",
    "    keras.layers.GlobalAvgPool1D(), # CREATE 1D VECTOR USIGN EMBEDDING\n",
    "    \n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # FIRST HIDDEN LAYER WITH 32 NODES\n",
    "    keras.layers.Dropout(0.7),\n",
    "    \n",
    "    keras.layers.Dense(32, activation=tf.nn.relu), # FIRST HIDDEN LAYER WITH 32 NODES\n",
    "    keras.layers.Dropout(0.7),\n",
    "    \n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # USING SIGMOID FOR ACTIVATION\n",
    "])\n",
    "\n",
    "# GET THE MODEL CONFIGURATION\n",
    "model.summary()\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), # USING ADAPTIVE LEARNING RATE\n",
    "              loss = keras.losses.binary_crossentropy, # CATAGORICAL LOSS FUNCTION\n",
    "              metrics = [\"accuracy\"]) # ACCURACY AS OUR METRIC FOR EVALUATION\n",
    "\n",
    "# FIT MODEL TO TRAIN DATA\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 100, # SET MAX EPOCH\n",
    "          batch_size = 128, # NUMBER OF REVIEWS AT ONCE TO TRAIN\n",
    "          validation_data = (val_review, val_labels),\n",
    "          callbacks = cbk)\n",
    "\n",
    "# EVALUATE MODEL ON TEST DATA\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
