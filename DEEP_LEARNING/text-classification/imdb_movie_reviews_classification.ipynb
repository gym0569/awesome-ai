{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nityansuman/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into buckets\n",
    "(train_review, train_labels), (test_review, test_labels) = movie_review_data.load_data(num_words=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument 100000 sets the upper limit of the number of words to keep data small, else we can also download the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "# lets view our data\n",
    "print(len(train_review), len(train_labels))\n",
    "print(len(test_review), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "print(train_review[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry these are integrs denoting different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that not all reviews is of same length. We need to make every review of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get word index mapping so that we can get the exact review and not the codes\n",
    "word_index_map = movie_review_data.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index_map)) # this returns a dictionary of words along with their mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push codes by 3 places to add some custom key-value mappings for our ease\n",
    "word_index_map = {k:(v+3) for k,v in word_index_map.items()}\n",
    "\n",
    "# add custom indices\n",
    "word_index_map[\"<PAD>\"] = 0 # extra symbols added to making sure length of all reviews are same\n",
    "word_index_map[\"<START>\"] = 1 # start of the review\n",
    "word_index_map[\"<UNK>\"] = 2  # unknown\n",
    "word_index_map[\"<UNUSED>\"] = 3 # un-used\n",
    "\n",
    "# now lets reverse the dict so that its easy transform: index->word\n",
    "reverse_word_index_map = dict([(value, key) for (key, value) in word_index_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# now we need to decode the reviews\n",
    "# take all the codes, find the word for that code and join using a space character to get the review\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index_map.get(num, '?') for num in text])\n",
    "\n",
    "# test a sample\n",
    "print(train_review[0])\n",
    "print(decode_review(train_review[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we have our reviews. Next step would be to preprae the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network. Tensors need to be of same length. First thing we do is now to make the lengths of each of the reviews same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use padding to make lengths of each review same\n",
    "# we also limit the maximum size of the reviews to 256 characters\n",
    "# post padding adds extra characters at the end of the review\n",
    "train_review = keras.preprocessing.sequence.pad_sequences(train_review,\n",
    "                                                         value = word_index_map[\"<PAD>\"],\n",
    "                                                          maxlen = 256,\n",
    "                                                          padding = \"post\"\n",
    "                                                         )\n",
    "# we do the same for test data\n",
    "test_review = keras.preprocessing.sequence.pad_sequences(test_review,\n",
    "                                                         value = word_index_map[\"<PAD>\"],\n",
    "                                                          maxlen = 256,\n",
    "                                                          padding = \"post\"\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now the length of the reveiws are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see extra padding is added at the end of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build are model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000 # input shape is the vocabulary count for the movies\n",
    "\n",
    "# we develope a sequential model: stack based model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 16), # look up for the embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create a fixed length 1D vector\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu), # make value ranges between 0 - infinite\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # make value range from 0-1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          1600000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,600,289\n",
      "Trainable params: 1,600,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # you can also look up your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, this neural network has 2 hidden layers. First layer is the input layer and last layer which is mostly softmax or sigmoid in case of classification (atleast) is the output layer. Each node of the output layer will have some probability and the highest probability will decide the class of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile the model. In this phase itself we decide on loss function and evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss = keras.losses.binary_crossentropy,\n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation set and updating the train set too!\n",
    "# first 10'000 are validation reviews and rest are for training\n",
    "val_review = train_review[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "train_review = train_review[10000:]\n",
    "train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.6914 - acc: 0.5255 - val_loss: 0.6863 - val_acc: 0.5730\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 39us/step - loss: 0.6800 - acc: 0.6193 - val_loss: 0.6767 - val_acc: 0.5936\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 39us/step - loss: 0.6649 - acc: 0.7309 - val_loss: 0.6587 - val_acc: 0.7306\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 40us/step - loss: 0.6417 - acc: 0.7589 - val_loss: 0.6343 - val_acc: 0.7619\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 37us/step - loss: 0.6073 - acc: 0.7959 - val_loss: 0.5987 - val_acc: 0.7739\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 38us/step - loss: 0.5617 - acc: 0.8078 - val_loss: 0.5572 - val_acc: 0.7889\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 40us/step - loss: 0.5078 - acc: 0.8307 - val_loss: 0.5091 - val_acc: 0.8106\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 40us/step - loss: 0.4530 - acc: 0.8512 - val_loss: 0.4642 - val_acc: 0.8285\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 39us/step - loss: 0.4016 - acc: 0.8702 - val_loss: 0.4256 - val_acc: 0.8422\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 41us/step - loss: 0.3566 - acc: 0.8861 - val_loss: 0.3936 - val_acc: 0.8518\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 39us/step - loss: 0.3182 - acc: 0.8982 - val_loss: 0.3681 - val_acc: 0.8634\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 42us/step - loss: 0.2864 - acc: 0.9086 - val_loss: 0.3484 - val_acc: 0.8668\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 52us/step - loss: 0.2597 - acc: 0.9156 - val_loss: 0.3320 - val_acc: 0.8736\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 49us/step - loss: 0.2371 - acc: 0.9243 - val_loss: 0.3205 - val_acc: 0.8758\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 48us/step - loss: 0.2176 - acc: 0.9303 - val_loss: 0.3098 - val_acc: 0.8789\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 39us/step - loss: 0.2001 - acc: 0.9368 - val_loss: 0.3024 - val_acc: 0.8813\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 45us/step - loss: 0.1846 - acc: 0.9428 - val_loss: 0.2962 - val_acc: 0.8826\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 41us/step - loss: 0.1707 - acc: 0.9473 - val_loss: 0.2926 - val_acc: 0.8848\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 44us/step - loss: 0.1583 - acc: 0.9529 - val_loss: 0.2877 - val_acc: 0.8868\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 47us/step - loss: 0.1463 - acc: 0.9575 - val_loss: 0.2841 - val_acc: 0.8871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0xb1f71fac8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets fit our model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 20, # 20 times the whole data is trained\n",
    "          batch_size = 256, # 256 reviews at a time\n",
    "          validation_data = (val_review, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 28us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3046057162475586, 0.87404]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant evaluation results in two values: loss and accruacy respectively. As we can see that out training accuracy is 95%,  validation accuracy is 88% and testing accuracy is almost 87%! The gap shows that the model is overfitted a little!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
