{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nityansuman/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into buckets\n",
    "(train_review, train_labels), (test_review, test_labels) = movie_review_data.load_data(num_words=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument 100000 sets the upper limit of the number of words to keep data small, else we can also download the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "# lets view our data\n",
    "print(len(train_review), len(train_labels))\n",
    "print(len(test_review), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "print(train_review[0], train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry these are integrs denoting different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we are taking here will use word embeddings. In word embeddings, the reviews are converted into a series of codes which are mapped against n-number of words, in our case which is 100,000 as seen above. Words not present in the vocab are discarded but this makes reviews uneven more than they were before, so we use extra padding to make reviews of same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then these codes are fed to the network with the word mapping. 1D vector is created of fixed length.\n",
    "We use ReLU and sigmoid for activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that not all reviews is of same length. We need to make every review of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get word index mapping so that we can get the exact review and not the codes\n",
    "word_index_map = movie_review_data.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index_map)) # this returns a dictionary of words along with their mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we decode the codes and get the reviews back. Only for example. Our model will use a built in embedding system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push codes by 3 places to add some custom key-value mappings for our ease\n",
    "word_index_map = {k:(v+3) for k,v in word_index_map.items()}\n",
    "\n",
    "# add custom indices\n",
    "word_index_map[\"<PAD>\"] = 0 # extra symbols added to making sure length of all reviews are same\n",
    "word_index_map[\"<START>\"] = 1 # start of the review\n",
    "word_index_map[\"<UNK>\"] = 2  # unknown\n",
    "word_index_map[\"<UNUSED>\"] = 3 # un-used\n",
    "\n",
    "# now lets reverse the dict so that its easy transform: index->word\n",
    "reverse_word_index_map = dict([(value, key) for (key, value) in word_index_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# now we need to decode the reviews\n",
    "# take all the codes, find the word for that code and join using a space character to get the review\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index_map.get(num, '?') for num in text])\n",
    "\n",
    "# test a sample\n",
    "print(train_review[0])\n",
    "print(decode_review(train_review[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that we have our reviews. Next step would be to preprae the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews—the arrays of integers—must be converted to tensors before fed into the neural network. Tensors need to be of same length. First thing we do is now to make the lengths of each of the reviews same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use padding to make lengths of each review same\n",
    "# we also limit the maximum size of the reviews to 256 characters\n",
    "# post padding adds extra characters at the end of the review\n",
    "train_review = keras.preprocessing.sequence.pad_sequences(train_review,\n",
    "                                                          value = word_index_map[\"<PAD>\"],\n",
    "                                                          maxlen = 256,\n",
    "                                                          padding = \"post\"\n",
    "                                                         )\n",
    "# we do the same for test data\n",
    "test_review = keras.preprocessing.sequence.pad_sequences(test_review,\n",
    "                                                         value = word_index_map[\"<PAD>\"],\n",
    "                                                         maxlen = 256,\n",
    "                                                         padding = \"post\"\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "print(len(train_review[0]), len(train_review[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now the length of the reveiws are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see extra padding is added at the end of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build are model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation set and updating the train set too!\n",
    "# first 10'000 are validation reviews and rest are for training\n",
    "val_review = train_review[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "\n",
    "train_review = train_review[10000:]\n",
    "train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          1600000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,600,289\n",
      "Trainable params: 1,600,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 153us/step - loss: 0.6877 - acc: 0.5595 - val_loss: 0.6790 - val_acc: 0.5804\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 142us/step - loss: 0.6592 - acc: 0.7020 - val_loss: 0.6361 - val_acc: 0.7510\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.5831 - acc: 0.7857 - val_loss: 0.5417 - val_acc: 0.7978\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 123us/step - loss: 0.4676 - acc: 0.8385 - val_loss: 0.4419 - val_acc: 0.8342\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 0.3677 - acc: 0.8752 - val_loss: 0.3749 - val_acc: 0.8572\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 149us/step - loss: 0.2978 - acc: 0.8979 - val_loss: 0.3340 - val_acc: 0.8696\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 146us/step - loss: 0.2511 - acc: 0.9123 - val_loss: 0.3102 - val_acc: 0.8777\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 139us/step - loss: 0.2165 - acc: 0.9252 - val_loss: 0.2964 - val_acc: 0.8826\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 0.1889 - acc: 0.9367 - val_loss: 0.2879 - val_acc: 0.8854\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 140us/step - loss: 0.1661 - acc: 0.9451 - val_loss: 0.2812 - val_acc: 0.8879\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 143us/step - loss: 0.1462 - acc: 0.9530 - val_loss: 0.2791 - val_acc: 0.8886\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 147us/step - loss: 0.1289 - acc: 0.9614 - val_loss: 0.2807 - val_acc: 0.8865\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 146us/step - loss: 0.1146 - acc: 0.9672 - val_loss: 0.2765 - val_acc: 0.8903\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 147us/step - loss: 0.1013 - acc: 0.9718 - val_loss: 0.2769 - val_acc: 0.8914\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 143us/step - loss: 0.0903 - acc: 0.9752 - val_loss: 0.2783 - val_acc: 0.8926\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 145us/step - loss: 0.0789 - acc: 0.9809 - val_loss: 0.2808 - val_acc: 0.8924\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 140us/step - loss: 0.0701 - acc: 0.9827 - val_loss: 0.2852 - val_acc: 0.8920\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 148us/step - loss: 0.0620 - acc: 0.9858 - val_loss: 0.2907 - val_acc: 0.8905\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 168us/step - loss: 0.0549 - acc: 0.9880 - val_loss: 0.2963 - val_acc: 0.8916\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 149us/step - loss: 0.0490 - acc: 0.9899 - val_loss: 0.3010 - val_acc: 0.8888\n",
      "25000/25000 [==============================] - 1s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3299652925682068, 0.87596]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100000 # input shape is the vocabulary count for the movies\n",
    "\n",
    "# we develope a sequential model: stack based model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, 16), # look up for the embedding\n",
    "    keras.layers.GlobalAvgPool1D(), # create a fixed length 1D vector\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu), # make value ranges between 0 - infinite\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid) # make value range from 0-1\n",
    "])\n",
    "\n",
    "# get the model architecure\n",
    "model.summary()\n",
    "\n",
    "# compile the model: basically glue the model together\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss = keras.losses.binary_crossentropy,\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "# fit our model to the training data\n",
    "model.fit(train_review,\n",
    "          train_labels,\n",
    "          epochs = 20, # 20 times the whole data is trained\n",
    "          batch_size = 64, # 256 reviews at a time\n",
    "          validation_data = (val_review, val_labels)) # check performance on validation dataset\n",
    "\n",
    "# evaluate the model on the test data\n",
    "model.evaluate(test_review, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant evaluation results in two values: loss and accruacy respectively. As we can see that out training accuracy is way more than the validation accuracyand testing accuracy. The gap shows that the model is overfitting - model is learning non important patterns and not generalizing to work on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
